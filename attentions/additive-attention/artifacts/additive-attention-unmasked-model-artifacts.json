{"train_losses": [4.786647193757448, 3.913415067521486, 3.5514621409025486, 3.268223057759491, 3.0100675931586043, 2.788145825726345, 2.5915882723972135, 2.418353404242562, 2.2669812939765697, 2.1359284149917737, 2.023700751922204, 1.9214952853282643, 1.8268543124724064, 1.740153164065357, 1.6694188916210562, 1.5946941769595713, 1.5342009597937967, 1.4720193636049783, 1.413618461676106, 1.3594922809348757, 1.3096353277760981, 1.261008051523553, 1.2181254058157296, 1.175447293315165, 1.1373240015580266, 1.098042240489422, 1.0573933437007115], "val_losses": [4.044384449720383, 3.58266681432724, 3.2800037264823914, 3.0033936500549316, 2.7576533257961273, 2.5649842023849487, 2.403564751148224, 2.2740002274513245, 2.1627964675426483, 2.079709902405739, 2.0161029994487762, 1.9638011455535889, 1.9135368168354034, 1.8796988427639008, 1.8573071509599686, 1.835921972990036, 1.8193637281656265, 1.8070522546768188, 1.8047286719083786, 1.8009289801120758, 1.788626030087471, 1.7883299738168716, 1.8036384880542755, 1.7998909205198288, 1.8038515001535416, 1.7962755411863327, 1.822363555431366], "batch_size": 128, "learning_rate": 0.0005, "epochs": 30, "model": "AttentionSeq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(5422, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (hidden_projection): Linear(in_features=1024, out_features=512, bias=True)\n    (cell_projection): Linear(in_features=1024, out_features=512, bias=True)\n  )\n  (decoder): AttentionDecoder(\n    (embedding): Embedding(4560, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (attention): BahdanauAttention(\n      (query_projection): Linear(in_features=512, out_features=512, bias=True)\n      (key_projection): Linear(in_features=1024, out_features=512, bias=True)\n      (score_projection): Linear(in_features=512, out_features=1, bias=False)\n    )\n    (lstm): LSTM(1536, 512, num_layers=2, batch_first=True, dropout=0.3)\n    (output_projection): Linear(in_features=512, out_features=4560, bias=True)\n  )\n)", "model_path": "/content/drive/My Drive/ML study/Attentions/additive attention/additive-attention-model.pt"}