{"train_losses": [4.773691904177225, 3.890961680643359, 3.553047688522003, 3.2920746288635656, 3.051783389456997, 2.8415257825725404, 2.6465672007741383, 2.472677713973932, 2.3128533195293945, 2.1745520331260915, 2.051995475386733, 1.94568985306744, 1.8502931258751958, 1.763351976608915, 1.683791882141046, 1.6115846381838626, 1.5439950941942862, 1.477942021407745, 1.421715487992711, 1.370771039950165, 1.3161560822163385, 1.2642423402895486, 1.2205860084374045, 1.1753726029185996, 1.1320938008472259, 1.0895553871923607, 1.054890157630265], "val_losses": [4.005652904510498, 3.564823627471924, 3.286634862422943, 3.0535716116428375, 2.819550395011902, 2.6275559961795807, 2.461157411336899, 2.3139107525348663, 2.203504115343094, 2.101123496890068, 2.035291910171509, 1.9740708619356155, 1.9219280630350113, 1.8903994113206863, 1.8418866991996765, 1.8261735141277313, 1.8208324760198593, 1.7992541193962097, 1.795655369758606, 1.7857183665037155, 1.7767019420862198, 1.7594160437583923, 1.774618223309517, 1.778915286064148, 1.777682512998581, 1.7948384135961533, 1.7974661141633987], "batch_size": 128, "learning_rate": 0.0005, "epochs": 30, "model": "AttentionSeq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(5422, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (hidden_projection): Linear(in_features=1024, out_features=512, bias=True)\n    (cell_projection): Linear(in_features=1024, out_features=512, bias=True)\n  )\n  (decoder): AttentionDecoder(\n    (embedding): Embedding(4560, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (attention): BahdanauAttention(\n      (query_projection): Linear(in_features=512, out_features=512, bias=True)\n      (key_projection): Linear(in_features=1024, out_features=512, bias=True)\n      (score_projection): Linear(in_features=512, out_features=1, bias=False)\n    )\n    (lstm): LSTM(1536, 512, num_layers=2, batch_first=True, dropout=0.3)\n    (output_projection): Linear(in_features=512, out_features=4560, bias=True)\n  )\n)", "model_path": "/content/drive/My Drive/ML study/Attentions/additive attention/additive-attention-complete-model.pt"}