{"train_losses": [4.948296832618209, 4.171267389726008, 3.8230972342554166, 3.5682628868960076, 3.3469674282662143, 3.1446449578070954, 2.98566735788589, 2.861622912243074, 2.7465943670482886, 2.666623358159338, 2.5954090383084334, 2.526630713551055, 2.4728808171948673, 2.40814860381744, 2.3865935644914402, 2.354893538395214, 2.337694991527675, 2.326147134083483, 2.3005378367092115, 2.2674676148376802, 2.2393630653751053, 2.2105053589732635, 2.1714317241954384, 2.1699584585979648], "val_losses": [4.230050563812256, 3.69621542096138, 3.3921152353286743, 3.1212336719036102, 2.910872846841812, 2.726609468460083, 2.5619177520275116, 2.439756453037262, 2.358755052089691, 2.2751630544662476, 2.2264111936092377, 2.1641776114702225, 2.1372204422950745, 2.106866717338562, 2.101809173822403, 2.1073265969753265, 2.0795666873455048, 2.127143159508705, 2.130930632352829, 2.1521562039852142, 2.1466541290283203, 2.190619871020317, 2.2085037529468536, 2.2822637259960175], "val_bleu_scores": [0.036944467574357986, 0.05017855763435364, 0.06874970346689224, 0.09318225830793381, 0.12250852584838867, 0.14738240838050842, 0.17407092452049255, 0.19610799849033356, 0.2251233607530594, 0.24495697021484375, 0.26190832257270813, 0.2776861786842346, 0.28413891792297363, 0.2982785701751709, 0.2996291518211365, 0.30245691537857056, 0.311160683631897, 0.3070198595523834, 0.3043211102485657, 0.3091861307621002, 0.30965641140937805, 0.30627816915512085, 0.3097522258758545, 0.2988326847553253], "teacher_forcing_ratios": [0.87, 0.85, 0.83, 0.81, 0.79, 0.76, 0.73, 0.7, 0.67, 0.63, 0.6, 0.56, 0.52, 0.49, 0.45, 0.41, 0.38, 0.34, 0.31, 0.28, 0.25, 0.22, 0.2, 0.17], "batch_size": 128, "learning_rate": 0.0005, "epochs": 40, "model": "AttentionSeq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(5422, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (hidden_projection): Linear(in_features=1024, out_features=512, bias=True)\n    (cell_projection): Linear(in_features=1024, out_features=512, bias=True)\n  )\n  (decoder): AttentionDecoder(\n    (embedding): Embedding(4560, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (attention): BahdanauAttention(\n      (query_projection): Linear(in_features=512, out_features=512, bias=True)\n      (key_projection): Linear(in_features=1024, out_features=512, bias=True)\n      (score_projection): Linear(in_features=512, out_features=1, bias=False)\n    )\n    (lstm): LSTM(1536, 512, num_layers=2, batch_first=True, dropout=0.3)\n    (output_projection): Linear(in_features=512, out_features=4560, bias=True)\n  )\n)", "model_path": "/content/drive/My Drive/ML study/Attentions/additive attention/weights/additive-attention-scheduled-sampling-model.pt", "max_grad_norm": 1.0}