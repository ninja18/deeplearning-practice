{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249bc84683f06fb4",
   "metadata": {},
   "source": [
    "## Additive attention\n",
    "Paper: [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473) - Bahdanau et. al 2015\n",
    "\n",
    "Dataset: [Multi30K English to Deutsche dataset](https://huggingface.co/datasets/bentrevett/multi30k)\n",
    "\n",
    "Model: Use LSTM as encoder and decoder\n",
    "\n",
    "#### Model variations\n",
    "- Stacked LSTM encoder decoder\n",
    "- BiLSTM encoder + LSTM decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c25a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000\n",
      "{'en': 'Two young, White males are outside near many bushes.', 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}\n",
      "['en', 'de']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"bentrevett/multi30k\", split=\"train\")\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0])\n",
    "print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134b8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "def word_tokenize(text):\n",
    "    text = text.lower().strip()\n",
    "    return TOKEN_RE.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0677d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7980113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n",
    "\n",
    "SPECIAL_TOKENS = [PAD, BOS, EOS, UNK]\n",
    "\n",
    "def build_vocab(tokenized_texts, max_vocab_size=10000, min_freq=3):\n",
    "    counter = Counter(token for text in tokenized_texts for token in text)\n",
    "    vocab = SPECIAL_TOKENS.copy()\n",
    "\n",
    "    for token, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            break\n",
    "        if len(vocab) >= max_vocab_size:\n",
    "            break\n",
    "        if token not in vocab:\n",
    "            vocab.append(token)\n",
    "\n",
    "    vocab_to_index = {token: index for index, token in enumerate(vocab)}\n",
    "    index_to_vocab = {index: token for token, index in vocab_to_index.items()}\n",
    "\n",
    "    return vocab_to_index, index_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b69e87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(tokens):\n",
    "    return [BOS] + tokens + [EOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56900004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(tokens):\n",
    "    return [token for token in tokens if token not in [PAD, BOS, EOS]]\n",
    "\n",
    "def encode(token_to_index, text):\n",
    "    return [token_to_index.get(token, token_to_index[UNK]) for token in text]\n",
    "\n",
    "def decode(index_to_token, indices):\n",
    "    return \" \".join(remove_special_tokens([index_to_token.get(index, UNK) for index in indices]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02a28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and build vocab\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(lambda x: {\"en\": word_tokenize(x[\"en\"]), \"de\": word_tokenize(x[\"de\"])}, batched=False)\n",
    "\n",
    "en_vocab_to_index, en_index_to_vocab = build_vocab(\n",
    "    [item[\"en\"] for item in tokenized_train_dataset]\n",
    ")\n",
    "de_vocab_to_index, de_index_to_vocab = build_vocab(\n",
    "    [item[\"de\"] for item in tokenized_train_dataset]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2339a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab size: 4560\n",
      "German Vocab size: 5422\n",
      "{'en': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'], 'de': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(f\"English Vocab size: {len(en_vocab_to_index)}\")\n",
    "print(f\"German Vocab size: {len(de_vocab_to_index)}\")\n",
    "print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c84c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch, source_lang, target_lang, source_vocab_to_index, target_vocab_to_index):\n",
    "    source_encodings = [encode(source_vocab_to_index, add_special_tokens(word_tokenize(text))) for text in batch[source_lang]]\n",
    "    target_encodings = [encode(target_vocab_to_index, add_special_tokens(word_tokenize(text))) for text in batch[target_lang]]\n",
    "\n",
    "    return {\"source\": source_encodings, \"target\": target_encodings}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f8435e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': tensor([   1,   18,   27,  215,   31,   85,   20,   89,    7,   15,  115,    3,\n",
      "        3149,    4,    2]), 'target': tensor([   1,   16,   24,   15,   25,  776,   17,   57,   80,  204, 1305,    5,\n",
      "           2])}\n",
      "German: zwei junge weiße männer sind im freien in der nähe <unk> büsche .\n",
      "English: two young , white males are outside near many bushes .\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train_dataset = train_dataset.map(\n",
    "    lambda x: preprocess(x, \"de\", \"en\", de_vocab_to_index, en_vocab_to_index),\n",
    "    batched=True,\n",
    "    remove_columns=[\"en\", \"de\"]\n",
    ")\n",
    "preprocessed_train_dataset.set_format(type=\"torch\", columns=[\"source\", \"target\"])\n",
    "\n",
    "print(preprocessed_train_dataset[0])\n",
    "print(f\"German: {decode(de_index_to_vocab, preprocessed_train_dataset[0]['source'].tolist())}\")\n",
    "print(f\"English: {decode(en_index_to_vocab, preprocessed_train_dataset[0]['target'].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170a4a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "length: 29000\n",
      "{'source': tensor([   1,   18,   27,  215,   31,   85,   20,   89,    7,   15,  115,    3,\n",
      "        3149,    4,    2]), 'target': tensor([   1,   16,   24,   15,   25,  776,   17,   57,   80,  204, 1305,    5,\n",
      "           2])}\n",
      "{'source': tensor([   1,   77,   31,   11,  831, 2082,    5,    3,    4,    2]), 'target': tensor([   1,  113,   30,    6,  325,  280,   17, 1180,    4,  712, 3814, 2644,\n",
      "           5,    2])}\n",
      "{'source': tensor([  1,   5,  67,  26, 226,   7,   5,   3,  58, 492,   4,   2]), 'target': tensor([   1,    4,   53,   33,  231,   69,    4,  248, 3815,    5,    2])}\n",
      "{'source': tensor([  1,   5,  13,   7,   6,  47,  41,  30,  12,  14, 546,  10, 684,   5,\n",
      "        250,   4,   2]), 'target': tensor([  1,   4,   9,   6,   4,  29,  23,  10,  36,   8,   4, 574, 575,   4,\n",
      "        240,   5,   2])}\n"
     ]
    }
   ],
   "source": [
    "print(f\"type: {type(preprocessed_train_dataset)}\")\n",
    "print(f\"length: {len(preprocessed_train_dataset)}\")\n",
    "print(preprocessed_train_dataset[0])\n",
    "print(preprocessed_train_dataset[1])\n",
    "print(preprocessed_train_dataset[2])\n",
    "print(preprocessed_train_dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1fe5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_batch(sequences, pad_idx=0):\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    max_length = lengths.max().item()\n",
    "    padded_batch = torch.full((len(sequences), max_length), pad_idx)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = lengths[i]\n",
    "        padded_batch[i, :end] = seq\n",
    "    return padded_batch, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "939cecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    source = [item[\"source\"] for item in batch]\n",
    "    target = [item[\"target\"] for item in batch]\n",
    "\n",
    "    source, source_lengths = pad_batch(source) # defer padding till batching\n",
    "    target, target_lengths = pad_batch(target)\n",
    "\n",
    "    return {\"source\": source, \"source_lengths\": source_lengths, \"target\": target, \"target_lengths\": target_lengths}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9354221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 19])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(\n",
    "    preprocessed_train_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(batch[\"source\"].shape)\n",
    "print(batch[\"source_lengths\"].shape)\n",
    "print(batch[\"target\"].shape)\n",
    "print(batch[\"target_lengths\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "337936d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.directions = 2\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        self.hidden_projection = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.cell_projection = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, source_encodings, source_lengths): # source_encodings: (batch_size, max_length), source_lengths: (batch_size)\n",
    "        B, T = source_encodings.size()\n",
    "        h_0 = torch.zeros(self.num_layers * self.directions, B, self.hidden_dim)\n",
    "        c_0 = torch.zeros(self.num_layers * self.directions, B, self.hidden_dim)\n",
    "\n",
    "        embedded = self.embedding(source_encodings) # (B, T, embedding_dim)\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, source_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, (last_hidden, last_cell) = self.lstm(packed_embedded, (h_0, c_0)) # hidden: (num_layers * directions, B, hidden_dim)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True) # outputs: (B, T, hidden_dim * directions)\n",
    "\n",
    "        h_f = last_hidden[0::2] # (num_layers, B, hidden_dim)\n",
    "        h_b = last_hidden[1::2] # (num_layers, B, hidden_dim)\n",
    "        h_cat = torch.cat((h_f, h_b), dim=2) # (num_layers, B, hidden_dim * 2) concatenate along the hidden dimension\n",
    "\n",
    "        c_f = last_cell[0::2] # (num_layers, B, hidden_dim)\n",
    "        c_b = last_cell[1::2] # (num_layers, B, hidden_dim)\n",
    "        c_cat = torch.cat((c_f, c_b), dim=2) # (num_layers, B, hidden_dim * 2) concatenate along the hidden dimension\n",
    "\n",
    "        last_hidden = torch.tanh(self.hidden_projection(h_cat)) # (num_layers, B, hidden_dim)\n",
    "        last_cell = torch.tanh(self.cell_projection(c_cat)) # (num_layers, B, hidden_dim)\n",
    "\n",
    "        return outputs, last_hidden, last_cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6479c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs (B, T, hidden_dim * directions): torch.Size([3, 5, 64])\n",
      "last_hidden (num_layers * directions, B, hidden_dim): torch.Size([2, 3, 32])\n",
      "last_cell (num_layers * directions, B, hidden_dim): torch.Size([2, 3, 32])\n",
      "last_hidden: tensor([[[ 0.0788, -0.0284,  0.1819, -0.0922,  0.1050, -0.0100, -0.2240,\n",
      "           0.0224, -0.1046,  0.1614, -0.0043,  0.0925, -0.1726, -0.0251,\n",
      "          -0.1199, -0.0522,  0.0073, -0.1281, -0.0866, -0.1276,  0.1677,\n",
      "           0.0379, -0.1042, -0.1321, -0.1767, -0.2932, -0.1137, -0.0041,\n",
      "           0.0403,  0.1457, -0.0557, -0.0722],\n",
      "         [ 0.1437, -0.0456,  0.2158, -0.1223,  0.1345, -0.0075, -0.2380,\n",
      "           0.0491, -0.1186,  0.1785,  0.0361,  0.0780, -0.1970, -0.0717,\n",
      "          -0.0765, -0.0764,  0.0210, -0.1607, -0.0897, -0.1058,  0.1730,\n",
      "           0.0217, -0.1530, -0.1606, -0.1826, -0.3062, -0.0887,  0.0848,\n",
      "           0.0294,  0.1544, -0.0967, -0.0838],\n",
      "         [-0.1216,  0.0375,  0.1296, -0.0756,  0.0616, -0.0055, -0.1475,\n",
      "          -0.0135, -0.1208,  0.1795, -0.1002,  0.0768, -0.1313, -0.0337,\n",
      "          -0.0567,  0.0691,  0.0305, -0.0187, -0.0590, -0.1694,  0.0045,\n",
      "           0.0329, -0.0168, -0.0871, -0.1061, -0.1606, -0.1382, -0.0376,\n",
      "           0.0949,  0.1021,  0.0590, -0.0089]],\n",
      "\n",
      "        [[ 0.0661, -0.0024,  0.1181, -0.1573,  0.0894,  0.0890, -0.0284,\n",
      "           0.0656, -0.1022,  0.0499,  0.0594,  0.1039, -0.1041,  0.0149,\n",
      "          -0.0422, -0.0250,  0.0703, -0.0514, -0.1345, -0.0317, -0.1505,\n",
      "           0.1119, -0.1019,  0.0517, -0.0323, -0.1017, -0.0856, -0.2147,\n",
      "          -0.0483,  0.0448, -0.0105, -0.0403],\n",
      "         [ 0.0650, -0.0134,  0.1151, -0.1719,  0.0560,  0.0997, -0.0424,\n",
      "           0.0382, -0.0791,  0.0727,  0.0546,  0.0915, -0.1090,  0.0046,\n",
      "          -0.0326, -0.0228,  0.0789, -0.0446, -0.1275, -0.0575, -0.1235,\n",
      "           0.1134, -0.0943,  0.0237, -0.0148, -0.1005, -0.0947, -0.2155,\n",
      "          -0.0562,  0.0444,  0.0096, -0.0378],\n",
      "         [ 0.0600,  0.0016,  0.1012, -0.1251,  0.0907,  0.0620, -0.0664,\n",
      "           0.0828, -0.1076,  0.0532,  0.0344,  0.1445, -0.0935,  0.0159,\n",
      "          -0.0652, -0.0260,  0.0584, -0.0582, -0.1265, -0.0107, -0.1071,\n",
      "           0.0926, -0.0700,  0.0457, -0.0762, -0.0824, -0.0742, -0.1436,\n",
      "           0.0021,  0.0419, -0.0209, -0.0743]]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(5, 20, 32, 2, 0.0)\n",
    "\n",
    "random_source_encodings = torch.randint(0, 5, (3, 5))\n",
    "random_source_lengths = torch.tensor([5, 4, 3])\n",
    "\n",
    "outputs, last_hidden, last_cell = encoder(random_source_encodings, random_source_lengths)\n",
    "\n",
    "print(f\"outputs (B, T, hidden_dim * directions): {outputs.shape}\")\n",
    "print(f\"last_hidden (num_layers * directions, B, hidden_dim): {last_hidden.shape}\")\n",
    "print(f\"last_cell (num_layers * directions, B, hidden_dim): {last_cell.shape}\")\n",
    "\n",
    "print(f\"last_hidden: {last_hidden}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e96fdeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source lengths: tensor([14,  9, 20])\n",
      "outputs (B, T, hidden_dim * directions): torch.Size([3, 20, 512])\n",
      "last_hidden (num_layers * directions, B, hidden_dim): torch.Size([2, 3, 256])\n",
      "last_cell (num_layers * directions, B, hidden_dim): torch.Size([2, 3, 256])\n",
      "tensor([ 4.3524e-02,  1.5765e-02, -2.1352e-02, -1.8995e-02,  2.7187e-02,\n",
      "         6.0332e-02, -3.6823e-02,  5.0755e-02,  1.5351e-02, -2.9590e-02,\n",
      "        -1.3017e-02, -2.6168e-02, -9.1572e-03,  3.5017e-02,  4.3192e-02,\n",
      "         3.2589e-02,  3.3422e-02, -2.4076e-02, -1.2255e-02,  2.2332e-02,\n",
      "        -3.8486e-02, -2.3635e-02, -8.1174e-02, -8.9897e-02,  3.2353e-02,\n",
      "         3.5341e-02,  9.8116e-02, -1.1213e-02,  3.3061e-02, -2.4547e-02,\n",
      "        -2.5842e-02, -5.9218e-02,  4.0249e-03,  1.5704e-02,  1.2798e-02,\n",
      "         1.4629e-02, -1.2037e-02, -3.0187e-02, -4.5610e-02,  5.2475e-02,\n",
      "         3.7832e-02, -4.4547e-02, -1.7846e-02,  3.8613e-02,  3.1487e-02,\n",
      "         2.4184e-02,  1.9012e-02, -4.7324e-02,  4.3101e-02, -8.8056e-03,\n",
      "        -4.5978e-02, -3.1753e-02, -6.2893e-02,  9.0469e-02, -5.3563e-02,\n",
      "         1.5556e-03,  6.1922e-02,  6.2853e-02,  1.1530e-02,  4.9751e-02,\n",
      "        -1.4533e-02, -7.2254e-02, -4.0135e-02, -5.8945e-03,  2.5355e-02,\n",
      "        -5.6160e-02, -8.7986e-02,  9.5984e-03,  1.3090e-02,  7.0984e-02,\n",
      "         1.6422e-02,  3.5386e-02, -4.6117e-02, -2.4288e-02, -5.4953e-02,\n",
      "         7.1391e-03, -3.7447e-02,  2.4494e-02, -9.9749e-02,  4.0567e-02,\n",
      "        -8.7408e-02, -4.0632e-02,  9.5424e-02,  2.4369e-02, -8.4786e-03,\n",
      "         5.2173e-02,  1.9625e-03,  4.0400e-02, -2.9121e-02,  8.2732e-02,\n",
      "         5.0615e-02, -5.0781e-02, -2.5276e-02,  5.8473e-03,  3.5955e-02,\n",
      "        -4.7373e-02, -2.6830e-02, -6.4823e-02, -3.2310e-02,  1.7115e-02,\n",
      "        -4.7140e-02, -7.1848e-03,  3.9202e-02, -3.2036e-02,  5.6627e-03,\n",
      "         1.5533e-02, -4.6984e-02,  2.6326e-02, -5.8738e-02,  6.9498e-04,\n",
      "         5.9812e-02,  4.2604e-02,  3.8321e-02,  2.0657e-02, -7.1775e-02,\n",
      "        -5.6529e-02, -1.6180e-02, -5.7544e-02,  1.6647e-02,  6.7212e-02,\n",
      "        -2.4985e-02, -6.9972e-03, -7.8671e-03,  1.1414e-02,  3.9262e-02,\n",
      "         1.7839e-03, -1.1990e-02,  3.7225e-02, -2.7373e-03,  2.4642e-02,\n",
      "        -1.6468e-02,  8.5557e-02,  1.2016e-02,  2.3467e-02,  4.4409e-02,\n",
      "        -6.0607e-03,  4.5144e-02, -3.8780e-02,  2.7940e-02, -6.4779e-04,\n",
      "        -4.9952e-03,  5.9841e-02,  2.7939e-03, -7.4357e-02,  5.3836e-02,\n",
      "        -1.3380e-02, -3.2070e-02,  1.7580e-02, -9.2583e-02,  2.9119e-02,\n",
      "         1.3914e-02,  5.2483e-02,  3.5295e-02, -5.5904e-02, -5.3847e-02,\n",
      "         5.1337e-02,  1.6119e-02,  2.8647e-02, -2.1546e-02, -1.4388e-02,\n",
      "        -7.1909e-02, -3.0793e-02,  2.0183e-03, -1.2492e-02,  5.7924e-03,\n",
      "         3.0265e-02,  2.2020e-02, -6.0295e-02,  7.0494e-02,  8.4329e-03,\n",
      "        -5.6578e-03,  5.7614e-03,  2.1693e-02, -5.0073e-02,  1.5554e-02,\n",
      "         3.2635e-02, -4.8879e-02,  8.0480e-03,  3.8577e-02,  4.6621e-03,\n",
      "         1.2841e-02, -5.1008e-02,  4.7225e-02,  2.8218e-02, -4.0381e-02,\n",
      "        -7.2543e-03, -2.9459e-02,  4.9362e-02,  2.1415e-02, -4.6944e-02,\n",
      "        -7.4972e-02,  4.6833e-02, -6.9404e-02,  1.5921e-02, -7.2836e-02,\n",
      "        -1.5360e-02,  5.1675e-02, -1.0706e-01, -1.0332e-02,  1.5859e-02,\n",
      "        -1.4200e-02,  3.5033e-02,  4.8415e-02, -7.2938e-05,  2.0049e-02,\n",
      "        -3.7842e-03,  5.2527e-02,  5.5796e-02, -2.3749e-02,  7.1573e-02,\n",
      "         6.0002e-02,  4.6029e-02,  1.8447e-02, -1.5135e-03,  3.3351e-02,\n",
      "        -4.9557e-02, -5.4766e-02,  4.3744e-02,  8.1289e-02,  1.2119e-02,\n",
      "         4.0468e-03, -2.0602e-02, -9.0137e-03,  2.3283e-02,  6.5955e-02,\n",
      "        -2.0008e-02,  4.5466e-02,  2.6704e-02, -8.7931e-02, -5.0705e-03,\n",
      "        -2.5073e-02,  1.3807e-02,  3.0376e-02,  2.0101e-03, -7.2562e-02,\n",
      "        -3.4014e-02,  3.5075e-02,  2.7149e-03, -4.4951e-02,  5.1869e-02,\n",
      "         4.3481e-02, -4.3737e-02, -6.0777e-02, -4.2260e-02, -1.8102e-02,\n",
      "         4.6401e-04,  1.0481e-02, -2.2467e-02, -5.7447e-02,  4.4386e-02,\n",
      "        -9.1179e-02, -3.2351e-02,  2.7508e-02,  3.6477e-02, -1.4204e-02,\n",
      "        -4.5264e-03,  1.0852e-02, -7.7907e-02, -2.7084e-02,  3.0587e-02,\n",
      "         3.2350e-02,  1.0528e-04,  4.0961e-03,  2.4267e-02, -3.8107e-02,\n",
      "         3.2895e-02, -3.3231e-02,  3.7532e-02, -2.0806e-02, -1.0558e-02,\n",
      "         1.7095e-02, -3.2978e-02, -2.0103e-02,  3.9144e-02, -1.0915e-02,\n",
      "         8.3781e-03,  3.3367e-02,  2.4733e-02, -2.4234e-02,  1.2991e-03,\n",
      "         5.0152e-02,  3.7481e-02, -4.9791e-02,  1.3492e-02,  1.7438e-02,\n",
      "        -1.7480e-02,  2.4634e-02,  2.2127e-02,  3.2083e-02,  6.6637e-03,\n",
      "        -3.0382e-02, -2.7291e-02,  4.6226e-02, -4.1258e-02, -9.7965e-03,\n",
      "         2.8095e-02,  7.6624e-03,  4.4706e-03, -5.8515e-03,  5.0824e-02,\n",
      "         1.0879e-03,  9.3082e-03, -9.5264e-03, -4.2820e-03,  6.7773e-03,\n",
      "        -1.8242e-02, -1.1624e-02, -3.5848e-02, -4.4292e-03,  1.1307e-02,\n",
      "        -4.6349e-03, -2.5002e-02,  3.9828e-02,  2.8712e-02, -1.2109e-02,\n",
      "         3.9505e-03,  6.5725e-03,  3.1774e-02, -9.6608e-03,  6.3004e-03,\n",
      "        -2.2085e-02, -1.9644e-02,  1.9878e-06,  9.6455e-03,  2.7143e-04,\n",
      "         5.3166e-02,  3.0699e-02, -1.1578e-02,  8.5484e-03, -2.0539e-02,\n",
      "        -1.9522e-03,  9.1304e-04, -2.1523e-02, -3.2285e-02,  3.9574e-02,\n",
      "        -3.0787e-02, -4.0880e-02, -1.6369e-02, -9.4715e-03,  1.9514e-02,\n",
      "        -6.1183e-03,  3.6320e-02,  3.6691e-03,  3.1653e-02,  1.3595e-02,\n",
      "        -3.0037e-02,  1.7021e-02,  2.8706e-02, -4.4928e-02,  1.1169e-02,\n",
      "         6.9821e-03, -3.8803e-02, -3.3780e-02,  5.1382e-02,  1.3925e-03,\n",
      "        -5.3456e-03,  2.8126e-02, -4.3235e-03, -5.6977e-02,  6.8409e-03,\n",
      "         2.2682e-02, -3.8723e-02, -2.7294e-02, -1.9672e-02, -1.1760e-02,\n",
      "         3.3482e-02,  2.0651e-04, -3.9585e-02, -1.8357e-03,  3.0945e-02,\n",
      "         5.9795e-04,  1.2199e-02,  1.9841e-02, -1.6619e-02, -5.1273e-02,\n",
      "         3.5533e-02,  2.8313e-02, -2.9010e-02, -1.1350e-02, -1.1091e-02,\n",
      "        -4.3550e-03, -2.6304e-02, -4.2836e-02, -3.6025e-02, -3.6468e-02,\n",
      "         1.0189e-02, -3.2183e-02,  3.8488e-03,  4.9028e-02,  1.0355e-02,\n",
      "        -2.5048e-02, -1.3208e-03,  1.5885e-02, -6.9724e-03,  2.7491e-03,\n",
      "         1.7991e-03, -2.2817e-02, -4.5447e-03, -2.4378e-02,  2.0937e-02,\n",
      "         3.1230e-02, -1.5137e-02,  3.8609e-02, -3.5849e-02, -4.8348e-02,\n",
      "         4.5297e-02, -1.1208e-02, -8.5053e-04,  2.1810e-02, -1.4523e-02,\n",
      "        -1.8436e-02,  2.9486e-02,  1.5132e-02, -3.4638e-02,  1.7325e-02,\n",
      "         5.1301e-02, -3.3765e-04, -2.1228e-03, -2.7868e-02,  1.4524e-02,\n",
      "         1.8381e-02, -1.4896e-03,  1.2062e-04,  1.5023e-02,  3.5707e-02,\n",
      "        -4.0844e-02, -1.7202e-02,  4.2409e-03,  1.0264e-02, -3.2077e-02,\n",
      "        -7.7354e-04, -1.7635e-02, -1.6879e-02, -2.7495e-03,  9.8954e-03,\n",
      "         2.4249e-03, -6.8164e-03, -4.0346e-02, -1.8866e-02, -1.5005e-02,\n",
      "        -1.0978e-02, -3.2125e-02, -2.3653e-02,  1.8296e-02, -2.9964e-02,\n",
      "        -3.0009e-02,  1.7991e-02, -1.2519e-03, -4.7129e-02, -2.6835e-02,\n",
      "        -5.6219e-03, -1.4635e-02,  2.3213e-02, -3.5696e-02, -2.7619e-02,\n",
      "         3.9700e-03,  8.7187e-03,  1.3512e-02,  4.6264e-03,  4.6373e-02,\n",
      "        -1.7443e-02,  3.9249e-03,  2.1409e-02,  2.7684e-02, -8.8609e-03,\n",
      "         2.0095e-03, -2.2975e-03,  3.0001e-02, -1.2041e-02,  3.8040e-02,\n",
      "         1.3669e-02, -1.5552e-02, -1.6138e-02, -8.9719e-03,  1.1934e-03,\n",
      "         2.1375e-02, -1.2715e-03,  1.1610e-03,  1.7499e-02,  4.3499e-03,\n",
      "        -6.0828e-03,  9.6928e-03, -1.5105e-02,  1.8942e-02, -8.2861e-03,\n",
      "        -4.5416e-04,  3.7313e-02,  5.2790e-03, -1.7592e-02, -3.7367e-03,\n",
      "        -4.4072e-02,  2.5852e-02, -1.3102e-02,  3.0265e-03,  5.1938e-03,\n",
      "         1.1458e-02,  2.5098e-02, -1.2743e-02,  3.1375e-03,  5.0556e-03,\n",
      "        -1.8418e-03, -4.5602e-02,  5.7819e-02,  2.3790e-02,  3.1697e-02,\n",
      "        -5.4072e-03, -8.8514e-02,  2.4148e-02,  5.5306e-03,  1.3995e-02,\n",
      "        -2.4014e-02, -2.3071e-02], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(len(de_vocab_to_index), 120, 256, 2, 0.0)\n",
    "\n",
    "outputs, last_hidden, last_cell = encoder(batch[\"source\"], batch[\"source_lengths\"])\n",
    "\n",
    "print(f\"source lengths: {batch['source_lengths']}\")\n",
    "print(f\"outputs (B, T, hidden_dim * directions): {outputs.shape}\")\n",
    "print(f\"last_hidden (num_layers * directions, B, hidden_dim): {last_hidden.shape}\")\n",
    "print(f\"last_cell (num_layers * directions, B, hidden_dim): {last_cell.shape}\")\n",
    "\n",
    "print(outputs[0][13]) # hidden state for padded token after re padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1101b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder for the Seq2Seq model. This works on a batch of single step targets (B, 1).\n",
    "    It doesn't take the entire target sequence, because the decision to teacher force and what kind of search strategy to use\n",
    "    is done at the sequence level, not the step level.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        self.encoder_hidden_projection = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.encoder_cell_projection = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target, encoder_last_hidden, encoder_last_cell):\n",
    "        # Normalize target shape to (B, 1)\n",
    "        if target.dim() == 0:\n",
    "            target = target.view(1, 1)\n",
    "        else:\n",
    "            target = target.unsqueeze(1)\n",
    "\n",
    "        target = target.long()\n",
    "\n",
    "        embedded = self.embedding(target)  # (B, T, embedding_dim)\n",
    "        # use dropout on the embedded input later\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (encoder_last_hidden, encoder_last_cell)) # (B, 1, hidden_dim)\n",
    "        logits = self.output_projection(outputs) # (B, 1, vocab_size)\n",
    "\n",
    "        return logits, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d68b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) tensor([1, 1, 1])\n",
      "logits (B, 1, vocab_size): torch.Size([3, 1, 4560])\n",
      "hidden (num_layers, B, hidden_dim): torch.Size([2, 3, 256])\n",
      "cell (num_layers, B, hidden_dim): torch.Size([2, 3, 256])\n",
      "tensor([[[ 0.0064,  0.0087,  0.0349,  ..., -0.0150,  0.0367, -0.0055]],\n",
      "\n",
      "        [[ 0.0049,  0.0064,  0.0333,  ..., -0.0152,  0.0376, -0.0006]],\n",
      "\n",
      "        [[ 0.0094,  0.0095,  0.0435,  ..., -0.0171,  0.0397, -0.0075]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(len(en_vocab_to_index), 120, 256, 2, 0.0)\n",
    "\n",
    "# One decoding step for the whole batch (B,)\n",
    "step0 = batch[\"target\"][:, 0]\n",
    "print(step0.shape, step0)\n",
    "\n",
    "logits, hidden, cell = decoder(step0, last_hidden, last_cell)\n",
    "print(f\"logits (B, 1, vocab_size): {logits.shape}\")\n",
    "print(f\"hidden (num_layers, B, hidden_dim): {hidden.shape}\")\n",
    "print(f\"cell (num_layers, B, hidden_dim): {cell.shape}\")\n",
    "print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2198a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, bos_idx, eos_idx, max_target_length):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.max_target_length = max_target_length\n",
    "    def forward(self, source, source_lengths, target=None):\n",
    "        logits_all = []\n",
    "        preds_all = []\n",
    "        B = source.shape[0]\n",
    "\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(source, source_lengths)\n",
    "\n",
    "        if target is not None:\n",
    "            steps = target.shape[1] - 1\n",
    "            inputs = target[:, 0]\n",
    "        else:\n",
    "            steps = self.max_target_length\n",
    "            inputs = torch.full((B,), self.bos_idx, dtype=torch.long)\n",
    "\n",
    "\n",
    "        for t in range(steps):\n",
    "            logits, hidden, cell = self.decoder(inputs, hidden, cell)\n",
    "            step_logits = logits.squeeze(1) # (B, 1, vocab_size) -> (B, vocab_size)\n",
    "            step_preds = step_logits.argmax(dim=1)\n",
    "\n",
    "            logits_all.append(step_logits)\n",
    "            preds_all.append(step_preds)\n",
    "\n",
    "            # Teacher forcing\n",
    "            if target is not None:\n",
    "                inputs = target[:, t + 1]\n",
    "            else:\n",
    "                inputs = step_preds\n",
    "                if step_preds.eq(self.eos_idx).all():\n",
    "                    break\n",
    "\n",
    "\n",
    "        logits_all = torch.stack(logits_all, dim=1) # (B, steps, vocab_size)\n",
    "        preds_all = torch.stack(preds_all, dim=1) # (B, steps)\n",
    "\n",
    "        return logits_all, preds_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f958536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=de_vocab_to_index[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d49ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_all.shape: torch.Size([3, 18, 4560])\n",
      "preds_all.shape: torch.Size([3, 18])\n",
      "batch['target'].shape: torch.Size([3, 19])\n",
      "loss: 8.42668342590332\n",
      "\n",
      "Source: ein brauner hund läuft mit etwas in seinem mund durch wasser .\n",
      "Target: a brown dog <unk> the water with something in its mouth .\n",
      "Pred: reddish balanced he baker puck puck horn slowly garden garden puck feet feet passing passing skater skater skater\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source: zwei mädchen zeigen sich gegenseitig etwas .\n",
      "Target: two girls showing something to each other .\n",
      "Pred: floaties mic puck trots bra 6 puck puck puck squinting skater skater skater skater puck puck puck puck\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Source: eine frau kauft artikel von einem mann in einem roten hemd , der auf seinem hof verkauft .\n",
      "Target: a woman is purchasing items from the man in the red shirt ' s yard sale .\n",
      "Pred: balanced balanced puck puck country snorkel range save gown gown troops does does excitedly snorkel snorkel 5 trots\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(encoder, decoder, de_vocab_to_index[BOS], de_vocab_to_index[EOS], 30)\n",
    "\n",
    "logits_all, preds_all = model(batch[\"source\"], batch[\"source_lengths\"], batch[\"target\"])\n",
    "target = batch[\"target\"][:, 1:].reshape(-1)\n",
    "loss = criterion(logits_all.reshape(-1, logits_all.shape[-1]), target)\n",
    "\n",
    "\n",
    "print(f\"logits_all.shape: {logits_all.shape}\")\n",
    "print(f\"preds_all.shape: {preds_all.shape}\")\n",
    "print(f\"batch['target'].shape: {batch['target'].shape}\")\n",
    "print(f\"loss: {loss}\\n\")\n",
    "\n",
    "for i in range(len(batch.items())-1):\n",
    "    print(f\"Source: {decode(de_index_to_vocab, batch['source'][i].tolist())}\")\n",
    "    print(f\"Target: {decode(en_index_to_vocab, batch['target'][i].tolist())}\")\n",
    "    print(f\"Pred: {decode(en_index_to_vocab, preds_all[i].tolist())}\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12360bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_vocab_size = len(en_vocab_to_index) # 4560\n",
    "src_vocab_size = len(de_vocab_to_index) # 5422\n",
    "emb_dim = 512\n",
    "hidden_dim = 512\n",
    "enc_num_layers = 2\n",
    "dec_num_layers = 2\n",
    "enc_dropout = 0.0\n",
    "dec_dropout = 0.0\n",
    "padding_idx = de_vocab_to_index[PAD]\n",
    "bos_idx = de_vocab_to_index[BOS]\n",
    "eos_idx = de_vocab_to_index[EOS]\n",
    "max_target_length = 30\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3fc59161",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(src_vocab_size, emb_dim, hidden_dim, enc_num_layers, enc_dropout, padding_idx)\n",
    "decoder = Decoder(tgt_vocab_size, emb_dim, hidden_dim, dec_num_layers, dec_dropout, padding_idx)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, bos_idx, eos_idx, max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad40ae8",
   "metadata": {},
   "source": [
    "### Model parameter calculation\n",
    "\n",
    "#### Encoder\n",
    "- source embedding: vocab * emb_dim + bias\n",
    "- 2 layers BiLSTM:  \n",
    "    - direction * gates * ( W_x + W_h + b_x + b_h)\n",
    "    - direction * gates * ( W_h + W_h + b_h + b_h)\n",
    "- hidden projection: 2 * (2 * hidden_dim * hidden_dim + b_h)\n",
    "\n",
    "#### Decoder\n",
    "- target embedding: vocab * emb_dim + bias\n",
    "- 2 layers LSTM: \n",
    "    - gates * ( W_x + W_h + b_x + b_h)\n",
    "    - gates * ( W_x + W_h + b_x + b_h)\n",
    "- output projection: hidden_dim * target_vocab + b_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cab4278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder parameters: 13,334,830\n",
      "Decoder parameters: 8,905,632\n",
      "Seq2Seq parameters: 22,240,462\n"
     ]
    }
   ],
   "source": [
    "p_src_emd = src_vocab_size * emb_dim + src_vocab_size\n",
    "p_enc_lstm_1 = 2 * 4 * (emb_dim * hidden_dim + hidden_dim * hidden_dim + (4*hidden_dim) + (4*hidden_dim))\n",
    "p_enc_lstm_2 = 2 * 4 * (hidden_dim * hidden_dim + hidden_dim * hidden_dim + (4*hidden_dim) + (4*hidden_dim))\n",
    "p_enc_hidden_proj = 2 * (2 * hidden_dim * hidden_dim + hidden_dim)\n",
    "p_enc_cell_proj = 2 * (2 * hidden_dim * hidden_dim + hidden_dim)\n",
    "\n",
    "p_encoder = p_src_emd + p_enc_lstm_1 + p_enc_lstm_2 + p_enc_hidden_proj + p_enc_cell_proj\n",
    "\n",
    "print(f\"Encoder parameters: {p_encoder:,}\")\n",
    "\n",
    "p_tgt_emd = tgt_vocab_size * emb_dim + tgt_vocab_size\n",
    "p_dec_lstm_1 = 4 * (emb_dim * hidden_dim + hidden_dim * hidden_dim + (4*hidden_dim) + (4*hidden_dim))\n",
    "p_dec_lstm_2 = 4 * (hidden_dim * hidden_dim + hidden_dim * hidden_dim + (4*hidden_dim) + (4*hidden_dim))\n",
    "p_dec_output_proj = hidden_dim * tgt_vocab_size + tgt_vocab_size\n",
    "\n",
    "p_decoder = p_tgt_emd + p_dec_lstm_1 + p_dec_lstm_2 + p_dec_output_proj\n",
    "\n",
    "print(f\"Decoder parameters: {p_decoder:,}\")\n",
    "\n",
    "p_seq2seq = p_encoder + p_decoder\n",
    "\n",
    "print(f\"Seq2Seq parameters: {p_seq2seq:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d74caea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Seq2Seq model has 24,253,904 trainable parameters\n",
      "Encoder parameters: 14,327,808\n",
      "Decoder parameters: 9,926,096\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The Seq2Seq model has {count_parameters(model):,} trainable parameters')\n",
    "print(f\"Encoder parameters: {count_parameters(encoder):,}\")\n",
    "print(f\"Decoder parameters: {count_parameters(decoder):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9643c2",
   "metadata": {},
   "source": [
    "### Rough estimate of memory needed\n",
    "- long or float32 = 4 bytes\n",
    "- memory for model for Adam optimizer = 4 * mode parameters * 4 bytes\n",
    "- activations of LSTM = 6 * hidden_dim * num_layers * batch_size * seq_len * 4 bytes\n",
    "- activations embedding = batch_size * seq_len * embed_dim * 2\n",
    "- hidden projection activations = batch_size * seq_len * hidden_dim * 2\n",
    "- output project activation = batch_size * seq_len * tgt_vocab\n",
    "\n",
    "total = sum of above + 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c166f00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 462,906,592 bytes\n",
      "Model memory: 355,847,392 bytes\n",
      "Activations memory: 107,059,200 bytes\n",
      "Total required memory: 601,778,569.6 bytes\n"
     ]
    }
   ],
   "source": [
    "model_memory = 4 * 4 * p_seq2seq\n",
    "activations_memory = 6 * hidden_dim * (enc_num_layers + dec_num_layers) * batch_size * max_target_length * 4\n",
    "embedding_memory = batch_size * max_target_length * emb_dim * 2\n",
    "hidden_projection_memory = batch_size * max_target_length * hidden_dim * 2\n",
    "output_projection_memory = batch_size * max_target_length * tgt_vocab_size\n",
    "\n",
    "total_activations_memory = activations_memory + embedding_memory + hidden_projection_memory + output_projection_memory\n",
    "total_memory = model_memory + total_activations_memory\n",
    "\n",
    "\n",
    "print(f\"Total memory: {total_memory:,} bytes\")\n",
    "print(f\"Model memory: {model_memory:,} bytes\")\n",
    "print(f\"Activations memory: {activations_memory + embedding_memory + hidden_projection_memory + output_projection_memory:,} bytes\")\n",
    "print(f\"Total required memory: {total_memory * 1.3:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f79912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
