{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249bc84683f06fb4",
   "metadata": {},
   "source": [
    "## Additive attention\n",
    "Paper: [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473) - Bahdanau et. al 2015\n",
    "\n",
    "Dataset: [Multi30K English to Deutsche dataset](https://huggingface.co/datasets/bentrevett/multi30k)\n",
    "\n",
    "Model: Use LSTM as encoder and decoder\n",
    "\n",
    "#### Model variations\n",
    "- Stacked LSTM encoder decoder\n",
    "- BiLSTM encoder + LSTM decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c25a3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000\n",
      "{'en': 'Two young, White males are outside near many bushes.', 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}\n",
      "['en', 'de']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"bentrevett/multi30k\", split=\"train\")\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0])\n",
    "print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134b8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TOKEN_RE = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "def word_tokenize(text):\n",
    "    text = text.lower().strip()\n",
    "    return TOKEN_RE.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0677d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7980113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "PAD, BOS, EOS, UNK = \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"\n",
    "\n",
    "SPECIAL_TOKENS = [PAD, BOS, EOS, UNK]\n",
    "\n",
    "def build_vocab(tokenized_texts, max_vocab_size=10000, min_freq=3):\n",
    "    counter = Counter(token for text in tokenized_texts for token in text)\n",
    "    vocab = SPECIAL_TOKENS.copy()\n",
    "\n",
    "    for token, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            break\n",
    "        if len(vocab) >= max_vocab_size:\n",
    "            break\n",
    "        if token not in vocab:\n",
    "            vocab.append(token)\n",
    "\n",
    "    vocab_to_index = {token: index for index, token in enumerate(vocab)}\n",
    "    index_to_vocab = {index: token for token, index in vocab_to_index.items()}\n",
    "\n",
    "    return vocab_to_index, index_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69e87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(tokens):\n",
    "    return [BOS] + tokens + [EOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56900004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(tokens):\n",
    "    return [token for token in tokens if token not in [PAD, BOS, EOS]]\n",
    "\n",
    "def encode(token_to_index, text):\n",
    "    return [token_to_index.get(token, token_to_index[UNK]) for token in text]\n",
    "\n",
    "def decode(index_to_token, indices):\n",
    "    return \" \".join(remove_special_tokens([index_to_token.get(index, UNK) for index in indices]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02a28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and build vocab\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(lambda x: {\"en\": word_tokenize(x[\"en\"]), \"de\": word_tokenize(x[\"de\"])}, batched=False)\n",
    "\n",
    "en_vocab_to_index, en_index_to_vocab = build_vocab(\n",
    "    [item[\"en\"] for item in tokenized_train_dataset]\n",
    ")\n",
    "de_vocab_to_index, de_index_to_vocab = build_vocab(\n",
    "    [item[\"de\"] for item in tokenized_train_dataset]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2339a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab size: 4560\n",
      "German Vocab size: 5422\n",
      "{'en': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'], 'de': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(f\"English Vocab size: {len(en_vocab_to_index)}\")\n",
    "print(f\"German Vocab size: {len(de_vocab_to_index)}\")\n",
    "print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c84c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch, source_lang, target_lang, source_vocab_to_index, target_vocab_to_index):\n",
    "    source_encodings = [encode(source_vocab_to_index, add_special_tokens(word_tokenize(text))) for text in batch[source_lang]]\n",
    "    target_encodings = [encode(target_vocab_to_index, add_special_tokens(word_tokenize(text))) for text in batch[target_lang]]\n",
    "\n",
    "    return {\"source\": source_encodings, \"target\": target_encodings}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8435e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': tensor([   1,   18,   27,  215,   31,   85,   20,   89,    7,   15,  115,    3,\n",
      "        3149,    4,    2]), 'target': tensor([   1,   16,   24,   15,   25,  776,   17,   57,   80,  204, 1305,    5,\n",
      "           2])}\n",
      "English: two young , white males are outside near many bushes .\n",
      "German: zwei junge weiße männer sind im freien in der nähe <unk> büsche .\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train_dataset = train_dataset.map(\n",
    "    lambda x: preprocess(x, \"de\", \"en\", de_vocab_to_index, en_vocab_to_index),\n",
    "    batched=True,\n",
    "    remove_columns=[\"en\", \"de\"]\n",
    ")\n",
    "preprocessed_train_dataset.set_format(type=\"torch\", columns=[\"source\", \"target\"])\n",
    "\n",
    "print(preprocessed_train_dataset[0])\n",
    "print(f\"German: {decode(de_index_to_vocab, preprocessed_train_dataset[0]['source'].tolist())}\")\n",
    "print(f\"English: {decode(en_index_to_vocab, preprocessed_train_dataset[0]['target'].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "170a4a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "length: 29000\n",
      "{'source': tensor([   1,   18,   27,  215,   31,   85,   20,   89,    7,   15,  115,    3,\n",
      "        3149,    4,    2]), 'target': tensor([   1,   16,   24,   15,   25,  776,   17,   57,   80,  204, 1305,    5,\n",
      "           2])}\n",
      "{'source': tensor([   1,   77,   31,   11,  831, 2082,    5,    3,    4,    2]), 'target': tensor([   1,  113,   30,    6,  325,  280,   17, 1180,    4,  712, 3814, 2644,\n",
      "           5,    2])}\n",
      "{'source': tensor([  1,   5,  67,  26, 226,   7,   5,   3,  58, 492,   4,   2]), 'target': tensor([   1,    4,   53,   33,  231,   69,    4,  248, 3815,    5,    2])}\n",
      "{'source': tensor([  1,   5,  13,   7,   6,  47,  41,  30,  12,  14, 546,  10, 684,   5,\n",
      "        250,   4,   2]), 'target': tensor([  1,   4,   9,   6,   4,  29,  23,  10,  36,   8,   4, 574, 575,   4,\n",
      "        240,   5,   2])}\n"
     ]
    }
   ],
   "source": [
    "print(f\"type: {type(preprocessed_train_dataset)}\")\n",
    "print(f\"length: {len(preprocessed_train_dataset)}\")\n",
    "print(preprocessed_train_dataset[0])\n",
    "print(preprocessed_train_dataset[1])\n",
    "print(preprocessed_train_dataset[2])\n",
    "print(preprocessed_train_dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fe5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_batch(sequences, pad_idx=0):\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    max_length = lengths.max().item()\n",
    "    padded_batch = torch.full((len(sequences), max_length), pad_idx)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        end = lengths[i]\n",
    "        padded_batch[i, :end] = seq\n",
    "    return padded_batch, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "939cecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    source = [item[\"source\"] for item in batch]\n",
    "    target = [item[\"target\"] for item in batch]\n",
    "\n",
    "    source, source_lengths = pad_batch(source) # defer padding till batching\n",
    "    target, target_lengths = pad_batch(target)\n",
    "\n",
    "    return {\"source\": source, \"source_lengths\": source_lengths, \"target\": target, \"target_lengths\": target_lengths}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 18])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(\n",
    "    preprocessed_train_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(batch[\"source\"].shape)\n",
    "print(batch[\"source_lengths\"].shape)\n",
    "print(batch[\"target\"].shape)\n",
    "print(batch[\"target_lengths\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "337936d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.directions = 2\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, source_encodings, source_lengths): # source_encodings: (batch_size, max_length), source_lengths: (batch_size)\n",
    "        B, T = source_encodings.size()\n",
    "        h_0 = torch.zeros(self.num_layers * self.directions, B, self.hidden_dim)\n",
    "        c_0 = torch.zeros(self.num_layers * self.directions, B, self.hidden_dim)\n",
    "\n",
    "        embedded = self.embedding(source_encodings) # (B, T, embedding_dim)\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, source_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, (last_hidden, last_cell) = self.lstm(packed_embedded, (h_0, c_0)) # hidden: (num_layers * directions, B, hidden_dim)\n",
    "        all_hiddens, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True) # all_hiddens: (B, T, hidden_dim * directions)\n",
    "\n",
    "        return all_hiddens, last_hidden, last_cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6479c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_hiddens (B, T, hidden_dim * directions): torch.Size([3, 5, 64])\n",
      "last_hidden (num_layers * directions, B, hidden_dim): torch.Size([4, 3, 32])\n",
      "last_cell (num_layers * directions, B, hidden_dim): torch.Size([4, 3, 32])\n",
      "last_hidden: tensor([[[ 3.6276e-02, -1.6420e-01, -1.2759e-01,  1.0308e-01,  1.8728e-02,\n",
      "           5.7011e-02, -2.2099e-01, -3.6541e-02,  3.0304e-02,  7.3551e-02,\n",
      "           4.2598e-02, -4.3661e-02, -5.9864e-02, -4.8414e-03, -1.6167e-01,\n",
      "           3.8792e-02,  1.4658e-01, -2.2599e-02,  6.9806e-02,  1.1741e-01,\n",
      "           6.7002e-02,  5.4525e-02, -6.4591e-02,  1.6797e-02, -1.6648e-01,\n",
      "          -1.1272e-01, -2.4655e-02, -9.3729e-02,  4.4855e-02,  7.0224e-04,\n",
      "           1.2342e-02,  1.7151e-02],\n",
      "         [-2.3649e-02, -1.0516e-01,  9.1599e-02,  1.5202e-01, -1.0838e-01,\n",
      "           7.9911e-02, -1.0120e-01,  6.1571e-02, -1.6074e-03,  3.2993e-01,\n",
      "          -1.2820e-01,  3.3278e-02,  9.7030e-02, -9.8207e-02, -1.5009e-01,\n",
      "          -1.0774e-01,  7.8268e-02,  1.2507e-01,  8.8591e-02,  1.6657e-01,\n",
      "          -1.5582e-01,  2.9137e-01,  1.0689e-02,  2.4880e-02,  3.4656e-03,\n",
      "          -1.9714e-01, -8.2071e-02, -7.0129e-02,  1.1612e-01, -2.8762e-02,\n",
      "          -4.8126e-02, -4.4465e-03],\n",
      "         [ 3.2384e-02, -1.0208e-01, -7.8058e-02,  5.7754e-02,  4.9461e-02,\n",
      "           8.4006e-02, -1.5645e-01, -1.3035e-01,  1.9557e-02,  9.5412e-02,\n",
      "           7.5712e-02,  4.7642e-03,  2.3240e-02,  4.3386e-02, -9.0526e-02,\n",
      "           4.1103e-02,  9.6279e-02,  4.2673e-02, -1.0854e-02,  6.0968e-02,\n",
      "           1.6779e-02,  9.6274e-02, -8.6150e-02, -5.3946e-03, -1.4031e-01,\n",
      "          -3.9933e-02,  3.5103e-03, -6.2543e-02,  1.0584e-01,  8.8111e-02,\n",
      "          -4.6987e-02, -1.4435e-02]],\n",
      "\n",
      "        [[-1.1145e-01, -3.9054e-02, -3.1766e-01, -4.9969e-02, -1.4758e-01,\n",
      "           9.9032e-02, -1.1247e-01,  1.3584e-01,  2.5908e-02,  2.5066e-01,\n",
      "          -1.2255e-01, -1.7489e-01, -3.6979e-02,  4.0781e-03, -1.8939e-02,\n",
      "           2.0480e-01, -9.1678e-02,  1.0943e-02,  2.0831e-01,  7.0957e-02,\n",
      "           4.6626e-03,  1.2601e-01,  1.7976e-01,  1.3495e-01, -1.1412e-01,\n",
      "           6.6951e-02,  1.0978e-01, -4.9348e-02, -8.7528e-02,  2.7855e-02,\n",
      "           8.1794e-02,  8.3779e-02],\n",
      "         [-1.2860e-01, -1.3321e-01, -4.4125e-01, -7.4963e-02, -7.3079e-02,\n",
      "           2.5117e-01, -3.5508e-02,  2.5674e-01,  1.3546e-01,  2.5307e-01,\n",
      "           8.6665e-02, -1.3812e-01, -7.5496e-02, -7.6586e-02,  2.8284e-02,\n",
      "           2.8066e-01, -2.0494e-01,  1.3049e-01,  1.4695e-01, -2.7971e-02,\n",
      "           4.4334e-02,  2.1754e-02,  2.0882e-01,  2.0034e-01, -1.1608e-01,\n",
      "           1.2022e-02,  1.8402e-01, -5.7770e-02, -5.0596e-02,  1.8219e-02,\n",
      "           1.4150e-01,  9.8552e-02],\n",
      "         [-8.4316e-02, -1.6263e-01, -3.1940e-01, -1.0772e-02, -7.2638e-02,\n",
      "           2.3409e-01, -6.5377e-02,  1.8568e-01, -4.4773e-03,  2.6999e-01,\n",
      "           7.7262e-02, -1.7158e-01, -7.2332e-02, -3.7807e-02, -1.3398e-01,\n",
      "           1.5923e-01, -2.0417e-02,  1.1793e-01, -9.2015e-02,  3.3660e-02,\n",
      "           4.5385e-02,  1.1759e-01,  1.7590e-01,  3.8993e-02, -8.0809e-02,\n",
      "          -7.1137e-02,  1.9432e-01,  8.7893e-04, -1.8293e-01,  1.1605e-01,\n",
      "           6.5400e-02,  8.5546e-02]],\n",
      "\n",
      "        [[ 4.4088e-02, -9.1721e-02, -5.5734e-02, -8.8399e-02, -1.2740e-02,\n",
      "           6.4441e-03, -4.7976e-02,  2.9830e-03, -1.5900e-02, -9.2098e-02,\n",
      "          -2.4643e-02, -1.3208e-01, -1.5927e-02,  5.5058e-02, -1.1267e-01,\n",
      "          -6.1950e-02,  7.5821e-02, -7.4333e-02, -7.5614e-02,  1.9874e-01,\n",
      "          -6.8278e-02, -1.2335e-02,  8.9427e-02,  3.0147e-02,  6.2895e-03,\n",
      "          -3.9775e-02,  1.8895e-01, -1.3517e-02, -4.6643e-02, -4.4003e-02,\n",
      "          -1.6545e-02, -7.4822e-02],\n",
      "         [ 2.0325e-02, -8.8286e-02, -1.1630e-02, -6.5388e-02, -4.7090e-02,\n",
      "          -7.3034e-03, -2.4735e-02,  1.1073e-02, -4.4570e-02, -9.4521e-02,\n",
      "          -5.8760e-02, -1.0673e-01,  1.2016e-02,  3.0765e-02, -1.7594e-01,\n",
      "          -1.0132e-01,  8.9861e-02, -6.5225e-02, -9.9694e-02,  1.2263e-01,\n",
      "          -5.8815e-02, -2.8606e-02,  2.6892e-02, -1.8583e-02,  2.2491e-02,\n",
      "          -7.4810e-02,  1.8295e-01, -4.1209e-02, -3.6293e-04, -3.2323e-02,\n",
      "          -4.2336e-02, -4.0636e-02],\n",
      "         [ 3.3061e-02, -8.2162e-02, -5.0365e-02, -8.6601e-02, -4.4598e-03,\n",
      "           1.0662e-02, -6.6017e-02,  2.1785e-02,  2.3159e-02, -5.9739e-02,\n",
      "          -3.5581e-02, -1.1935e-01, -1.3906e-02, -1.0746e-02, -7.1001e-02,\n",
      "          -6.8173e-02,  5.9530e-02, -2.5504e-02, -6.5971e-02,  1.7858e-01,\n",
      "          -7.4050e-02, -1.6216e-03,  7.7409e-02,  3.6054e-02, -1.8391e-03,\n",
      "          -3.4093e-02,  1.5614e-01, -8.2201e-03, -1.7481e-02, -3.9256e-02,\n",
      "          -1.8928e-02, -4.6697e-02]],\n",
      "\n",
      "        [[-2.2785e-02,  2.1521e-02,  4.0527e-02, -7.5922e-02, -1.1848e-01,\n",
      "           1.4551e-02, -2.1458e-02, -6.7489e-02,  1.2057e-01, -4.2604e-02,\n",
      "          -6.2625e-02,  5.7278e-05, -7.8513e-02,  3.9089e-02, -1.9654e-01,\n",
      "          -5.5518e-02, -1.0833e-02,  1.5827e-02,  2.3859e-02, -1.6516e-02,\n",
      "           4.6912e-02,  4.8489e-02,  1.1304e-01,  3.6666e-02,  1.0086e-01,\n",
      "           2.7088e-02,  4.5508e-02, -1.5362e-01,  4.4741e-02,  5.3369e-02,\n",
      "           2.2746e-03, -1.5886e-02],\n",
      "         [-4.9224e-02, -1.7342e-02,  9.8827e-03, -6.7838e-02, -9.3099e-02,\n",
      "           1.4410e-02, -1.2382e-02, -4.0644e-02,  1.2534e-01, -2.0473e-02,\n",
      "          -5.8949e-02,  2.5684e-02, -6.3075e-02,  2.5276e-02, -1.4263e-01,\n",
      "          -6.4426e-02, -3.0568e-02,  2.1579e-03,  2.7521e-02,  7.9989e-03,\n",
      "           3.3136e-02,  7.2642e-02,  8.2332e-02,  1.2826e-02,  9.9074e-02,\n",
      "           7.5960e-03,  5.1945e-02, -1.7064e-01,  3.2898e-02,  3.6334e-02,\n",
      "          -3.9549e-02, -7.7959e-03],\n",
      "         [-3.4426e-02, -1.9402e-02, -3.5041e-03, -3.4899e-02, -7.7682e-02,\n",
      "           3.2063e-02,  2.9717e-02, -4.3473e-03,  9.1779e-02, -3.1551e-02,\n",
      "          -4.1817e-02,  4.7932e-02, -6.9887e-02,  2.3510e-03, -1.3660e-01,\n",
      "          -7.2657e-02,  1.5215e-02,  1.6067e-02,  4.3341e-02, -2.5390e-02,\n",
      "           9.3987e-02,  8.0350e-02,  9.5416e-02,  1.6060e-02,  8.1384e-02,\n",
      "           2.6622e-02,  3.1688e-02, -8.2126e-02,  5.1888e-02,  6.7507e-02,\n",
      "           1.3685e-02,  3.2470e-02]]], grad_fn=<IndexSelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(5, 20, 32, 2, 0.0)\n",
    "\n",
    "random_source_encodings = torch.randint(0, 5, (3, 5))\n",
    "random_source_lengths = torch.tensor([5, 4, 3])\n",
    "\n",
    "all_hiddens, last_hidden, last_cell = encoder(random_source_encodings, random_source_lengths)\n",
    "\n",
    "print(f\"all_hiddens (B, T, hidden_dim * directions): {all_hiddens.shape}\")\n",
    "print(f\"last_hidden (num_layers * directions, B, hidden_dim): {last_hidden.shape}\")\n",
    "print(f\"last_cell (num_layers * directions, B, hidden_dim): {last_cell.shape}\")\n",
    "\n",
    "print(f\"last_hidden: {last_hidden}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e96fdeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source lengths: tensor([11, 14, 13])\n",
      "all_hiddens (B, T, hidden_dim * directions): torch.Size([3, 14, 512])\n",
      "last_hidden (num_layers * directions, B, hidden_dim): torch.Size([4, 3, 256])\n",
      "last_cell (num_layers * directions, B, hidden_dim): torch.Size([4, 3, 256])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(len(de_vocab_to_index), 120, 256, 2, 0.0)\n",
    "\n",
    "all_hiddens, last_hidden, last_cell = encoder(batch[\"source\"], batch[\"source_lengths\"])\n",
    "\n",
    "print(f\"source lengths: {batch['source_lengths']}\")\n",
    "print(f\"all_hiddens (B, T, hidden_dim * directions): {all_hiddens.shape}\")\n",
    "print(f\"last_hidden (num_layers * directions, B, hidden_dim): {last_hidden.shape}\")\n",
    "print(f\"last_cell (num_layers * directions, B, hidden_dim): {last_cell.shape}\")\n",
    "\n",
    "print(all_hiddens[0][13]) # hidden state for padded token after re padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1101b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True, bidirectional=False)\n",
    "        self.encoder_hidden_projection = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.encoder_cell_projection = nn.Linear(2*hidden_dim, hidden_dim)\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_encodings, target_lengths, encoder_outputs, encoder_last_hidden, encoder_last_cell):\n",
    "        h_0, c_0 = self._get_initial_hidden_state(encoder_last_hidden, encoder_last_cell)\n",
    "\n",
    "        embedded = self.embedding(target_encodings) # (B, T, embedding_dim)\n",
    "        outputs, (h_t, c_t) = self.lstm(embedded, (h_0, c_0)) # (B, T, hidden_dim)\n",
    "        logits = self.output_projection(outputs) # (B, T, vocab_size)\n",
    "        log_softmax = F.log_softmax(logits, dim=2) # (B, T, vocab_size)\n",
    "\n",
    "        return log_softmax\n",
    "\n",
    "    def _get_initial_hidden_state(self, encoder_hidden):\n",
    "        h_f = encoder_hidden[0::2] # (num_layers, B, hidden_dim)\n",
    "        h_b = encoder_hidden[1::2] # (num_layers, B, hidden_dim)\n",
    "        h_cat = torch.cat((h_f, h_b), dim=2) # (num_layers, B, hidden_dim * 2) concatenate along the hidden dimension\n",
    "        return torch.tanh(self.encoder_hidden_projection(h_cat)), torch.tanh(self.encoder_cell_projection(h_cat)) # (num_layers, B, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488269b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
