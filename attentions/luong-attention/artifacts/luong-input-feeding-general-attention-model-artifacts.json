{"train_losses": [4.911687510654264, 3.9586757909884014, 3.4535551974426806, 3.0374644054715327, 2.75609222907852, 2.565318693673558, 2.433815048654699, 2.3559991693706763, 2.285158550161622, 2.212280871059401, 2.1703791219232365, 2.193735016600151, 2.148475929503924, 2.12279055717233, 2.140348182900887, 2.1230897357285285, 2.1026300375682143, 2.1184685818424307, 2.109678550963885, 2.1027821045089925, 2.124662387213518, 2.096405190518249, 2.084350895776623, 2.0725321465126743], "val_losses": [3.991526275873184, 3.406565010547638, 2.90837761759758, 2.5141367614269257, 2.2980683743953705, 2.167152389883995, 2.0344417840242386, 1.9801445305347443, 1.9101613461971283, 1.8682597130537033, 1.8193768858909607, 1.8127931356430054, 1.796204388141632, 1.7943255454301834, 1.8141968250274658, 1.7932694256305695, 1.7796557694673538, 1.810571774840355, 1.854689508676529, 1.8375184535980225, 1.8487308323383331, 1.9143833667039871, 1.9335090965032578, 1.929014191031456], "val_bleu_scores": [0.04856019467115402, 0.07370447367429733, 0.14202815294265747, 0.21407081186771393, 0.25839608907699585, 0.2874274253845215, 0.3016967177391052, 0.32584625482559204, 0.32877597212791443, 0.34598231315612793, 0.35033097863197327, 0.34918397665023804, 0.3498154878616333, 0.3498568534851074, 0.3555785119533539, 0.35482609272003174, 0.359681099653244, 0.34644946455955505, 0.34663641452789307, 0.35578060150146484, 0.3485346734523773, 0.3405943512916565, 0.3311247229576111, 0.339877188205719], "teacher_forcing_ratios": [0.87, 0.85, 0.83, 0.81, 0.79, 0.76, 0.73, 0.7, 0.67, 0.63, 0.6, 0.56, 0.52, 0.49, 0.45, 0.41, 0.38, 0.34, 0.31, 0.28, 0.25, 0.22, 0.2, 0.17], "batch_size": 128, "learning_rate": 0.0005, "epochs": 40, "model": "AttentionSeq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(5422, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (hidden_projection): Linear(in_features=1024, out_features=512, bias=True)\n    (cell_projection): Linear(in_features=1024, out_features=512, bias=True)\n  )\n  (decoder): LuongAttentionInputFeedingDecoder(\n    (embedding): Embedding(4560, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (attention): GeneralAttention(\n      (W_a): Linear(in_features=512, out_features=1024, bias=False)\n    )\n    (lstm): LSTM(1024, 512, num_layers=2, batch_first=True, dropout=0.3)\n    (attention_projection): Linear(in_features=1536, out_features=512, bias=True)\n    (output_projection): Linear(in_features=512, out_features=4560, bias=True)\n  )\n)", "model_path": "/content/drive/My Drive/ML/Attentions/luong attention/weights/luong-input-feeding-general-attention-model.pt", "max_grad_norm": 1.0}