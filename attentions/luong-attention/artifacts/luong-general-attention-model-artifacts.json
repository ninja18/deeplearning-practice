{"train_losses": [4.844744877668204, 3.738836683365742, 3.222344566546873, 2.884626194243914, 2.6505890008111357, 2.5075206683070648, 2.3859329617496106, 2.3023352370913335, 2.253797846218563, 2.202561615847281, 2.1705907000319025, 2.168593386196355, 2.1622128712448254, 2.1039201577854576, 2.147018068162355, 2.1336137670777444, 2.0966908154508617, 2.10940016051221, 2.1349077293001084, 2.130093830797641, 2.1495323391212766], "val_losses": [3.8618496656417847, 3.115232288837433, 2.689700573682785, 2.426450252532959, 2.2249094247817993, 2.1005124151706696, 2.008576214313507, 1.9356660693883896, 1.8775990456342697, 1.8397077471017838, 1.8059958815574646, 1.8263206779956818, 1.8037665337324142, 1.7697888612747192, 1.7889102846384048, 1.7727926522493362, 1.7754857540130615, 1.7968864142894745, 1.8298878371715546, 1.8459567874670029, 1.8627252876758575], "val_bleu_scores": [0.05037688836455345, 0.13161955773830414, 0.19818222522735596, 0.24256494641304016, 0.26764601469039917, 0.2901460826396942, 0.3158281147480011, 0.32829031348228455, 0.33870765566825867, 0.34331014752388, 0.3543097972869873, 0.3438025712966919, 0.3568762540817261, 0.35424143075942993, 0.3563302159309387, 0.3547719717025757, 0.3543489873409271, 0.35509583353996277, 0.35239869356155396, 0.34956151247024536, 0.3409416675567627], "teacher_forcing_ratios": [0.87, 0.85, 0.83, 0.81, 0.79, 0.76, 0.73, 0.7, 0.67, 0.63, 0.6, 0.56, 0.52, 0.49, 0.45, 0.41, 0.38, 0.34, 0.31, 0.28, 0.25], "batch_size": 128, "learning_rate": 0.0005, "epochs": 40, "model": "AttentionSeq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(5422, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n    (hidden_projection): Linear(in_features=1024, out_features=512, bias=True)\n    (cell_projection): Linear(in_features=1024, out_features=512, bias=True)\n  )\n  (decoder): LuongAttentionDecoder(\n    (embedding): Embedding(4560, 512, padding_idx=0)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (attention): GeneralAttention(\n      (W_a): Linear(in_features=512, out_features=1024, bias=False)\n    )\n    (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.3)\n    (attention_projection): Linear(in_features=1536, out_features=512, bias=True)\n    (output_projection): Linear(in_features=512, out_features=4560, bias=True)\n  )\n)", "model_path": "/content/drive/My Drive/ML/Attentions/luong attention/weights/luong-general-attention-model.pt", "max_grad_norm": 1.0}